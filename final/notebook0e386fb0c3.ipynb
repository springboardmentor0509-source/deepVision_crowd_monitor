{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":492536,"sourceType":"datasetVersion","datasetId":230545}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport glob\nimport cv2\nimport numpy as np\nfrom scipy.io import loadmat\n\nprint(\"Environment Ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:17:58.232882Z","iopub.execute_input":"2025-12-05T06:17:58.233063Z","iopub.status.idle":"2025-12-05T06:17:58.784208Z","shell.execute_reply.started":"2025-12-05T06:17:58.233046Z","shell.execute_reply":"2025-12-05T06:17:58.783388Z"}},"outputs":[{"name":"stdout","text":"Environment Ready\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"SRC = \"/kaggle/input/shanghaitech/ShanghaiTech\"\nDST = \"/kaggle/working/shanghaitech\"\n\nif not os.path.exists(DST):\n    shutil.copytree(SRC, DST)\n\nprint(\"Dataset copied to:\", DST)\nprint(os.listdir(DST))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:18:08.524879Z","iopub.execute_input":"2025-12-05T06:18:08.525147Z","iopub.status.idle":"2025-12-05T06:18:35.703399Z","shell.execute_reply.started":"2025-12-05T06:18:08.525126Z","shell.execute_reply":"2025-12-05T06:18:35.702741Z"}},"outputs":[{"name":"stdout","text":"Dataset copied to: /kaggle/working/shanghaitech\n['part_B', 'part_A']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"DATA_ROOT = \"/kaggle/working/shanghaitech\"\n\nPART_A_TRAIN = f\"{DATA_ROOT}/part_A/train_data\"\nPART_A_TEST  = f\"{DATA_ROOT}/part_A/test_data\"\n\nprint(\"Train data:\", os.listdir(PART_A_TRAIN))\nprint(\"Test data:\", os.listdir(PART_A_TEST))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:18:40.123182Z","iopub.execute_input":"2025-12-05T06:18:40.124318Z","iopub.status.idle":"2025-12-05T06:18:40.129295Z","shell.execute_reply.started":"2025-12-05T06:18:40.124281Z","shell.execute_reply":"2025-12-05T06:18:40.128638Z"}},"outputs":[{"name":"stdout","text":"Train data: ['images', 'ground-truth']\nTest data: ['images', 'ground-truth']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def get_counts(folder):\n    img_dir = os.path.join(folder, \"images\")\n    gt_dir = os.path.join(folder, \"ground-truth\")\n\n    img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n    counts = []\n\n    for img_path in img_paths:\n        base = os.path.basename(img_path).replace(\".jpg\", \"\")\n        mat_path = os.path.join(gt_dir, f\"GT_{base}.mat\")\n\n        mat = loadmat(mat_path)\n\n        \n        points = mat[\"image_info\"][0][0][0][0][0]\n\n        counts.append(len(points))\n\n    return img_paths, np.array(counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:18:43.053966Z","iopub.execute_input":"2025-12-05T06:18:43.054236Z","iopub.status.idle":"2025-12-05T06:18:43.060259Z","shell.execute_reply.started":"2025-12-05T06:18:43.054215Z","shell.execute_reply":"2025-12-05T06:18:43.059355Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom skimage.feature import local_binary_pattern\nfrom skimage.feature import hog\n\ndef extract_features(img_paths, size=128):\n    \"\"\"\n    Extract combined visual features for each image:\n    - Resized grayscale pixels\n    - HOG features\n    - LBP features\n    - Edge density\n    - Brightness stats\n    - ORB keypoint count\n    \"\"\"\n    features = []\n\n    # LBP params\n    radius = 2\n    n_points = 8 * radius\n\n    for p in img_paths:\n        img = cv2.imread(p)\n        if img is None:\n            print(\"Could not read:\", p)\n            continue\n        \n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        gray_small = cv2.resize(gray, (size, size))\n\n       \n        #Flattened pixel intensities\n        raw_pix = gray_small.flatten()\n\n        # HOG descriptor \n        hog_feat = hog(\n            gray_small,\n            orientations=8,\n            pixels_per_cell=(16, 16),\n            cells_per_block=(1, 1),\n            visualize=False,\n            channel_axis=None\n        )\n        # LBP texture histogram\n        lbp = local_binary_pattern(gray_small, n_points, radius, method=\"uniform\")\n        lbp_hist, _ = np.histogram(lbp.ravel(), bins=int(lbp.max() + 1), density=True)\n\n        # Edge Density (Canny)\n        edges = cv2.Canny(gray_small, 100, 200)\n        edge_density = np.sum(edges > 0) / edges.size  # 0–1\n\n        # Brightness stats\n        mean_intensity = np.mean(gray_small)\n        var_intensity  = np.var(gray_small)\n\n        # ORB keypoints count\n\n        orb = cv2.ORB_create()\n        keypoints = orb.detect(gray_small, None)\n        kpt_count = len(keypoints)\n\n\n        # Combine all features\n        feat_vector = np.concatenate([\n            raw_pix,           # pixel values\n            hog_feat,          # structure\n            lbp_hist,          # texture\n            [edge_density],    # edges\n            [mean_intensity, var_intensity],\n            [kpt_count],       # keypoints\n        ])\n\n        features.append(feat_vector)\n\n    return np.array(features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:18:46.396602Z","iopub.execute_input":"2025-12-05T06:18:46.396862Z","iopub.status.idle":"2025-12-05T06:18:46.487977Z","shell.execute_reply.started":"2025-12-05T06:18:46.396840Z","shell.execute_reply":"2025-12-05T06:18:46.487416Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\n# Load image paths & GT counts\ntrain_img_paths, train_counts = get_counts(PART_A_TRAIN)\ntest_img_paths,  test_counts  = get_counts(PART_A_TEST)\n\nprint(\"Train images:\", len(train_img_paths), \" Test images:\", len(test_img_paths))\n\n\n# Extract features\nprint(\"Extracting training features...\")\ntrain_features = extract_features(train_img_paths)\n\nprint(\"Extracting test features...\")\ntest_features  = extract_features(test_img_paths)\n\n\n# 3. Convert to X, y\ntrain_X = train_features\ntrain_y = train_counts\n\ntest_X  = test_features\ntest_y  = test_counts\n\nprint(\"Shapes:\")\nprint(\"train_X:\", train_X.shape)\nprint(\"train_y:\", train_y.shape)\nprint(\"test_X:\", test_X.shape)\nprint(\"test_y:\", test_y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:18:51.778890Z","iopub.execute_input":"2025-12-05T06:18:51.779681Z","iopub.status.idle":"2025-12-05T06:19:00.192777Z","shell.execute_reply.started":"2025-12-05T06:18:51.779653Z","shell.execute_reply":"2025-12-05T06:19:00.192031Z"}},"outputs":[{"name":"stdout","text":"Train images: 300  Test images: 182\nExtracting training features...\nExtracting test features...\nShapes:\ntrain_X: (300, 16918)\ntrain_y: (300,)\ntest_X: (182, 16918)\ntest_y: (182,)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nimport joblib\n\nrf = RandomForestRegressor(\n    n_estimators=200,\n    max_depth=20,\n    random_state=42,\n    n_jobs=-1\n)\n\nrf.fit(train_X, train_y)\nprint(\"Random Forest training completed!\")\n\njoblib.dump(rf, \"/kaggle/working/random_forest_model.pkl\")\nprint(\"Model saved as random_forest_model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:20:48.098776Z","iopub.execute_input":"2025-12-05T06:20:48.099787Z","iopub.status.idle":"2025-12-05T06:23:19.313673Z","shell.execute_reply.started":"2025-12-05T06:20:48.099756Z","shell.execute_reply":"2025-12-05T06:23:19.312839Z"}},"outputs":[{"name":"stdout","text":"Random Forest training completed!\nModel saved as random_forest_model.pkl\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\n\n# 1. Predict on train & test\npred_train = rf.predict(train_X)\npred_test  = rf.predict(test_X)\n\n\n# Metrics\nmae_train = mean_absolute_error(train_y, pred_train)\nrmse_train = np.sqrt(mean_squared_error(train_y, pred_train))\nr2_train = r2_score(train_y, pred_train)\n\nmae_test = mean_absolute_error(test_y, pred_test)\nrmse_test = np.sqrt(mean_squared_error(test_y, pred_test))\nr2_test = r2_score(test_y, pred_test)\n\n\n# Print summary\n\nprint(\"========== Random Forest Evaluation ==========\")\nprint(f\"Train MAE :  {mae_train:.3f}\")\nprint(f\"Train RMSE: {rmse_train:.3f}\")\nprint(f\"Train R²  : {r2_train:.3f}\")\nprint(\"----------------------------------------------\")\nprint(f\"Test  MAE : {mae_test:.3f}\")\nprint(f\"Test  RMSE: {rmse_test:.3f}\")\nprint(f\"Test R²   : {r2_test:.3f}\")\nprint(\"==============================================\")\n\n\n# 4. Show some predictions\n\nfor i in range(5):\n    print(f\"Image: {os.path.basename(test_img_paths[i])} | GT: {test_y[i]} | Pred: {pred_test[i]:.1f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:51:42.854606Z","iopub.execute_input":"2025-12-05T06:51:42.855157Z","iopub.status.idle":"2025-12-05T06:51:42.967161Z","shell.execute_reply.started":"2025-12-05T06:51:42.855132Z","shell.execute_reply":"2025-12-05T06:51:42.966557Z"}},"outputs":[{"name":"stdout","text":"========== Random Forest Evaluation ==========\nTrain MAE :  107.853\nTrain RMSE: 155.522\nTrain R²  : 0.905\n----------------------------------------------\nTest  MAE : 237.064\nTest  RMSE: 296.678\nTest R²   : 0.296\n==============================================\nImage: IMG_1.jpg | GT: 172 | Pred: 575.9\nImage: IMG_10.jpg | GT: 502 | Pred: 540.8\nImage: IMG_100.jpg | GT: 389 | Pred: 463.0\nImage: IMG_101.jpg | GT: 211 | Pred: 500.0\nImage: IMG_102.jpg | GT: 223 | Pred: 549.4\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# -----------------------------\n# 1. Save Metrics to CSV\n# -----------------------------\nmetrics_df = pd.DataFrame({\n    \"Metric\": [\"Train MAE\", \"Train RMSE\", \"Train R²\", \"Test MAE\", \"Test RMSE\", \"Test R²\"],\n    \"Value\": [mae_train, rmse_train, r2_train, mae_test, rmse_test, r2_test]\n})\n\nmetrics_df.to_csv(\"random_forest_metrics.csv\", index=False)\nprint(\"Saved: random_forest_metrics.csv\")\n\n# -----------------------------\n# 2. Save first 5 predictions to CSV\n# -----------------------------\npred_df = pd.DataFrame({\n    \"Image\": [os.path.basename(p) for p in test_img_paths[:5]],\n    \"Ground Truth\": test_y[:5],\n    \"Prediction\": pred_test[:5]\n})\n\npred_df.to_csv(\"random_forest_predictions.csv\", index=False)\nprint(\"Saved: random_forest_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:51:59.232104Z","iopub.execute_input":"2025-12-05T06:51:59.232451Z","iopub.status.idle":"2025-12-05T06:51:59.511901Z","shell.execute_reply.started":"2025-12-05T06:51:59.232427Z","shell.execute_reply":"2025-12-05T06:51:59.511141Z"}},"outputs":[{"name":"stdout","text":"Saved: random_forest_metrics.csv\nSaved: random_forest_predictions.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# -----------------------------------------\n# 1. Scatter plot: Ground Truth vs Predicted Count\n# -----------------------------------------\nplt.figure(figsize=(6,6))\nplt.scatter(test_y, pred_test, alpha=0.6)\nplt.plot([test_y.min(), test_y.max()], \n         [test_y.min(), test_y.max()], \n         linestyle='--')\nplt.xlabel(\"Ground Truth Count\")\nplt.ylabel(\"Predicted Count\")\nplt.title(\"GT vs Predicted (Scatter Plot)\")\nplt.grid(True)\nplt.savefig(\"scatter_gt_pred.png\", dpi=300, bbox_inches='tight')\nplt.close()\n\n# -----------------------------------------\n# 2. Error Histogram\n# -----------------------------------------\nerrors = pred_test - test_y\n\nplt.figure(figsize=(8,5))\nplt.hist(errors, bins=40)\nplt.xlabel(\"Prediction Error (Pred - GT)\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Error Distribution\")\nplt.grid(True)\nplt.savefig(\"error_histogram.png\", dpi=300, bbox_inches='tight')\nplt.close()\n\n# -----------------------------------------\n# 3. Line Plot for first 50 images: GT vs Pred\n# -----------------------------------------\nN = min(50, len(test_y))\nplt.figure(figsize=(12,5))\nplt.plot(range(N), test_y[:N], label=\"Ground Truth\")\nplt.plot(range(N), pred_test[:N], label=\"Predicted\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Crowd Count\")\nplt.title(\"GT vs Pred for First 50 Samples\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"lineplot_gt_pred_50.png\", dpi=300, bbox_inches='tight')\nplt.close()\n\n# -----------------------------------------\n# 4. Residual Plot\n# -----------------------------------------\nplt.figure(figsize=(8,5))\nplt.scatter(test_y, errors, alpha=0.6)\nplt.axhline(0, color='black', linestyle='--')\nplt.xlabel(\"Ground Truth Count\")\nplt.ylabel(\"Residual (Pred - GT)\")\nplt.title(\"Residual Plot\")\nplt.grid(True)\nplt.savefig(\"residual_plot.png\", dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(\"All plots saved successfully as PNG files!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:52:16.233309Z","iopub.execute_input":"2025-12-05T06:52:16.233652Z","iopub.status.idle":"2025-12-05T06:52:17.902553Z","shell.execute_reply.started":"2025-12-05T06:52:16.233632Z","shell.execute_reply":"2025-12-05T06:52:17.901827Z"}},"outputs":[{"name":"stdout","text":"All plots saved successfully as PNG files!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"markdown","source":"### Simple CNN Model ","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom scipy.io import loadmat\nfrom scipy.ndimage import gaussian_filter\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# ============================================================\n# Create save directory\n# ============================================================\nSAVE_DIR = \"/kaggle/working/s_cnn\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n\n# ============================================================\n# 1. Density Map Generator\n# ============================================================\ndef generate_density_map(shape, points, sigma=1.5):\n    H, W = shape\n    density = np.zeros((H, W), dtype=np.float32)\n\n    for p in points:\n        x, y = int(p[0]), int(p[1])\n        if 0 <= x < W and 0 <= y < H:\n            density[y, x] = 1.0\n\n    return gaussian_filter(density, sigma=sigma, mode=\"constant\")\n\n\n# ============================================================\n# 2. CSRNet Dataset\n# ============================================================\nclass CSRNetDataset(Dataset):\n    def __init__(self, root_dir, part='A', mode='train',\n                 target_size=(512, 512), downsample_ratio=8):\n\n        self.img_dir = os.path.join(root_dir, f\"part_{part}\", f\"{mode}_data\", \"images\")\n        self.gt_dir = os.path.join(root_dir, f\"part_{part}\", f\"{mode}_data\", \"ground-truth\")\n\n        self.img_paths = sorted(glob.glob(os.path.join(self.img_dir, \"*.jpg\")))\n        self.target_size = target_size\n        self.downsample_ratio = downsample_ratio\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n        ])\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n\n        orig_w, orig_h = img.size\n        img_resized = img.resize(self.target_size[::-1], Image.BILINEAR)\n\n        # Load MATLAB GT\n        filename = os.path.basename(img_path).replace(\".jpg\", \"\")\n        mat_path = os.path.join(self.gt_dir, f\"GT_{filename}.mat\")\n\n        if os.path.exists(mat_path):\n            mat = loadmat(mat_path)\n            points = mat[\"image_info\"][0][0][0][0][0]\n        else:\n            points = np.array([])\n\n        # Scale points\n        scale_x = self.target_size[1] / orig_w\n        scale_y = self.target_size[0] / orig_h\n        if len(points) > 0:\n            points[:,0] *= scale_x\n            points[:,1] *= scale_y\n\n        # Downsample\n        out_h = self.target_size[0] // self.downsample_ratio\n        out_w = self.target_size[1] // self.downsample_ratio\n\n        points_out = points.copy()\n        if len(points_out) > 0:\n            points_out /= self.downsample_ratio\n\n        density = generate_density_map((out_h, out_w), points_out, sigma=2.0)\n        density_tensor = torch.from_numpy(density).unsqueeze(0)\n\n        img_tensor = self.transform(img_resized)\n\n        return img_tensor, density_tensor, img_path\n\n\n# ============================================================\n# 3. Simple CNN Model\n# ============================================================\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Conv2d(128,64,3,padding=1), nn.ReLU(),\n            nn.Conv2d(64,32,3,padding=1), nn.ReLU(),\n            nn.Conv2d(32,1,1)\n        )\n\n    def forward(self,x):\n        return self.decoder(self.encoder(x))\n\n\n# ============================================================\n# 4. Training + Evaluation + Saving\n# ============================================================\ndef train_csrnet():\n    DATA_ROOT = \"/kaggle/input/shanghaitech/ShanghaiTech\"\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    LR = 1e-4\n    BATCH_SIZE = 4\n    EPOCHS = 20\n\n    print(\"Using device:\", DEVICE)\n\n    train_loader = DataLoader(CSRNetDataset(DATA_ROOT, 'A', 'train'),\n                              batch_size=BATCH_SIZE, shuffle=True)\n\n    val_loader = DataLoader(CSRNetDataset(DATA_ROOT, 'A', 'test'),\n                            batch_size=1, shuffle=False)\n\n    model = SimpleCNN().to(DEVICE)\n\n    # Save architecture\n    with open(os.path.join(SAVE_DIR, \"simplecnn_architecture.txt\"), \"w\") as f:\n        f.write(str(model))\n\n    criterion = nn.MSELoss(reduction=\"sum\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n    train_losses = []\n    val_maes = []\n\n    best_mae = float(\"inf\")\n    last_preds = []\n    last_gts = []\n\n    for epoch in range(EPOCHS):\n\n        # ---------------- TRAINING ----------------\n        model.train()\n        total_loss = 0\n\n        for img, target, _ in train_loader:\n            img = img.to(DEVICE)\n            target = target.to(DEVICE)\n\n            pred = model(img)\n            loss = criterion(pred, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        train_losses.append(total_loss)\n\n        # ---------------- VALIDATION ----------------\n        model.eval()\n        mae = 0\n        mse = 0\n        last_preds = []\n        last_gts = []\n\n        with torch.no_grad():\n            for img, target, _ in val_loader:\n                img, target = img.to(DEVICE), target.to(DEVICE)\n                out = model(img)\n\n                pred_count = out.sum().item()\n                gt_count = target.sum().item()\n\n                last_preds.append(pred_count)\n                last_gts.append(gt_count)\n\n                mae += abs(pred_count - gt_count)\n                mse += (pred_count - gt_count) ** 2\n\n        mae /= len(val_loader)\n        rmse = (mse / len(val_loader)) ** 0.5\n\n        val_maes.append(mae)\n\n        print(f\"Epoch {epoch+1}/{EPOCHS} → TrainLoss={total_loss:.2f} | MAE={mae:.2f} | RMSE={rmse:.2f}\")\n\n        # Save best model\n        if mae < best_mae:\n            best_mae = mae\n            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_simplecnn.pth\")\n            print(\"✔ Best model saved!\")\n\n    # ============================================================\n    # SAVE METRICS CSV\n    # ============================================================\n    df = pd.DataFrame({\n        \"Epoch\": list(range(1, EPOCHS+1)),\n        \"Train_Loss\": train_losses,\n        \"Val_MAE\": val_maes\n    })\n    df.to_csv(os.path.join(SAVE_DIR, \"training_metrics.csv\"), index=False)\n\n    # ============================================================\n    # PLOTS\n    # ============================================================\n    # 1️⃣ Loss & MAE curve\n    plt.figure()\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_maes, label=\"Validation MAE\")\n    plt.legend()\n    plt.title(\"Training Curve\")\n    plt.savefig(os.path.join(SAVE_DIR, \"training_curve.png\"))\n    plt.close()\n\n    # 2️⃣ GT vs Pred\n    plt.figure()\n    plt.scatter(last_gts, last_preds)\n    plt.xlabel(\"GT\")\n    plt.ylabel(\"Pred\")\n    plt.title(\"GT vs Pred\")\n    plt.savefig(os.path.join(SAVE_DIR, \"gt_vs_pred.png\"))\n    plt.close()\n\n    # 3️⃣ Error Histogram\n    errors = np.array(last_preds) - np.array(last_gts)\n    plt.figure()\n    plt.hist(errors, bins=40)\n    plt.title(\"Error Histogram\")\n    plt.savefig(os.path.join(SAVE_DIR, \"error_histogram.png\"))\n    plt.close()\n\n    print(\"All training completed & plots saved.\")\n\n\n# ============================================================\n# 5. Single Image Prediction\n# ============================================================\ndef predict_single_image(model_path, image_path):\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = SimpleCNN().to(DEVICE)\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n    model.eval()\n\n    transform = transforms.Compose([\n        transforms.Resize((512,512)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n    ])\n\n    img = Image.open(image_path).convert(\"RGB\")\n    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        out = model(img_tensor)\n        count = out.sum().item()\n\n    print(f\"Predicted Count = {count:.2f}\")\n    return count\n\n\n# ============================================================\n# RUN TRAINING\n# ============================================================\ntrain_csrnet()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T09:38:04.255374Z","iopub.execute_input":"2025-12-05T09:38:04.255672Z","iopub.status.idle":"2025-12-05T09:41:27.103689Z","shell.execute_reply.started":"2025-12-05T09:38:04.255651Z","shell.execute_reply":"2025-12-05T09:41:27.103093Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1/20 → TrainLoss=28135.95 | MAE=264.95 | RMSE=300.04\n✔ Best model saved!\nEpoch 2/20 → TrainLoss=23089.57 | MAE=182.81 | RMSE=221.09\n✔ Best model saved!\nEpoch 3/20 → TrainLoss=19057.91 | MAE=156.73 | RMSE=203.41\n✔ Best model saved!\nEpoch 4/20 → TrainLoss=17364.91 | MAE=177.63 | RMSE=224.58\nEpoch 5/20 → TrainLoss=16114.19 | MAE=178.26 | RMSE=228.99\nEpoch 6/20 → TrainLoss=16225.85 | MAE=151.39 | RMSE=197.93\n✔ Best model saved!\nEpoch 7/20 → TrainLoss=15728.47 | MAE=151.02 | RMSE=202.14\n✔ Best model saved!\nEpoch 8/20 → TrainLoss=14575.40 | MAE=137.41 | RMSE=179.23\n✔ Best model saved!\nEpoch 9/20 → TrainLoss=14445.64 | MAE=205.95 | RMSE=252.29\nEpoch 10/20 → TrainLoss=13747.37 | MAE=135.12 | RMSE=185.40\n✔ Best model saved!\nEpoch 11/20 → TrainLoss=13189.56 | MAE=130.76 | RMSE=176.14\n✔ Best model saved!\nEpoch 12/20 → TrainLoss=12499.65 | MAE=254.23 | RMSE=326.97\nEpoch 13/20 → TrainLoss=12298.51 | MAE=148.99 | RMSE=216.91\nEpoch 14/20 → TrainLoss=11835.65 | MAE=220.25 | RMSE=288.50\nEpoch 15/20 → TrainLoss=11319.00 | MAE=143.23 | RMSE=205.56\nEpoch 16/20 → TrainLoss=10561.15 | MAE=150.76 | RMSE=193.20\nEpoch 17/20 → TrainLoss=10673.61 | MAE=174.26 | RMSE=239.24\nEpoch 18/20 → TrainLoss=10836.93 | MAE=141.10 | RMSE=198.99\nEpoch 19/20 → TrainLoss=11087.74 | MAE=142.99 | RMSE=183.89\nEpoch 20/20 → TrainLoss=10403.82 | MAE=196.51 | RMSE=243.29\nAll training completed & plots saved.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### CSRNet Model","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom scipy.ndimage import gaussian_filter\nfrom PIL import Image\nfrom scipy.io import loadmat\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nfrom datetime import datetime\n\n\n# -------------------------\n# Density Map Generation\n# -------------------------\ndef generate_density_map(shape, points, sigma=1.0):\n    \"\"\"\n    Generate a density map for a given shape and points.\n    shape: (H, W) of the output density map\n    points: [(x, y), ...] coordinates on the output density map scale\n    sigma: Gaussian sigma\n    \"\"\"\n    H, W = shape\n    density = np.zeros((H, W), dtype=np.float32)\n\n    # Filter points that are out of bounds\n    valid_points = []\n    for p in points:\n        x, y = int(p[0]), int(p[1])\n        if 0 <= x < W and 0 <= y < H:\n            density[y, x] = 1.0\n            valid_points.append(p)\n\n    # Apply Gaussian filter\n    density = gaussian_filter(density, sigma=sigma, mode='constant', cval=0)\n\n    return density\n\n\n# -------------------------\n# Dataset\n# -------------------------\nclass CSRNetDataset(Dataset):\n    def __init__(self, root_dir, part='A', mode='train', target_size=(512, 512), downsample_ratio=8):\n        \"\"\"\n        root_dir: Path to ShanghaiTech dataset (e.g., .../ShanghaiTech)\n        part: 'A' or 'B'\n        mode: 'train' or 'test'\n        target_size: (H, W) to resize input images to. CSRNet requires multiples of 16 usually.\n        downsample_ratio: 8 for VGG16-based CSRNet\n        \"\"\"\n        self.img_dir = os.path.join(root_dir, f\"part_{part}\", f\"{mode}_data\", \"images\")\n        self.gt_dir = os.path.join(root_dir, f\"part_{part}\", f\"{mode}_data\", \"ground-truth\")\n\n        self.img_paths = sorted(glob.glob(os.path.join(self.img_dir, \"*.jpg\")))\n        self.target_size = target_size\n        self.downsample_ratio = downsample_ratio\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n\n        # Load image\n        img = Image.open(img_path).convert('RGB')\n        orig_w, orig_h = img.size\n\n        # Resize image\n        img_resized = img.resize(self.target_size[::-1], Image.BILINEAR) # PIL uses (W, H)\n\n        # Load Ground Truth\n        filename = os.path.basename(img_path).replace(\".jpg\", \"\")\n        gt_path = os.path.join(self.gt_dir, f\"GT_{filename}.mat\")\n\n        if os.path.exists(gt_path):\n            mat = loadmat(gt_path)\n            # ShanghaiTech mat structure: image_info -> gt_count? image_info[0][0][0][0][0]\n            try:\n                points = mat[\"image_info\"][0][0][0][0][0]\n            except Exception:\n                # fallback: try different indexing\n                try:\n                    points = mat[\"image_info\"][0][0][0][0]\n                except Exception:\n                    points = np.array([])\n        else:\n            points = np.array([])\n\n        # Adjust points to resized image coordinates\n        scale_x = self.target_size[1] / orig_w\n        scale_y = self.target_size[0] / orig_h\n\n        points_resized = points.copy()\n        if len(points) > 0:\n            points_resized = points_resized.astype(np.float32)\n            points_resized[:, 0] *= scale_x\n            points_resized[:, 1] *= scale_y\n\n        # Generate Density Map (downsampled)\n        out_h = self.target_size[0] // self.downsample_ratio\n        out_w = self.target_size[1] // self.downsample_ratio\n\n        points_map = points_resized.copy()\n        if len(points) > 0:\n            points_map = points_map / float(self.downsample_ratio)\n\n        density = generate_density_map((out_h, out_w), points_map, sigma=2.0)\n\n        # Transform image\n        img_tensor = self.transform(img_resized)\n\n        # Density to tensor\n        density_tensor = torch.from_numpy(density).unsqueeze(0) # (1, H, W)\n\n        return img_tensor, density_tensor\n\n\n# -------------------------\n# Model\n# -------------------------\nclass CSRNet(nn.Module):\n    def __init__(self, load_weights=True):\n        super(CSRNet, self).__init__()\n        self.seen = 0\n        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n        self.backend_feat = [512, 512, 512, 256, 128, 64]\n\n        self.frontend = make_layers(self.frontend_feat)\n        self.backend = make_layers(self.backend_feat, in_channels=512, dilation=True)\n        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n\n        self._initialize_weights()\n\n        if load_weights:\n            # load pretrained vgg16 and copy matching conv weights\n            try:\n                mod = models.vgg16(pretrained=True)\n                frontend_layers = [l for l in self.frontend if isinstance(l, nn.Conv2d) or isinstance(l, nn.ReLU) or isinstance(l, nn.MaxPool2d)]\n                vgg_layers = list(mod.features.children())\n\n                i = 0\n                for layer in self.frontend:\n                    if isinstance(layer, nn.Conv2d):\n                        # advance to next conv in vgg\n                        while i < len(vgg_layers) and not isinstance(vgg_layers[i], nn.Conv2d):\n                            i += 1\n                        if i < len(vgg_layers) and isinstance(vgg_layers[i], nn.Conv2d):\n                            try:\n                                layer.weight.data.copy_(vgg_layers[i].weight.data)\n                                layer.bias.data.copy_(vgg_layers[i].bias.data)\n                            except Exception:\n                                pass\n                            i += 1\n            except Exception as e:\n                print(\"Warning: could not load pretrained VGG16 weights:\", e)\n\n    def forward(self, x):\n        x = self.frontend(x)\n        x = self.backend(x)\n        x = self.output_layer(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_layers(cfg, in_channels=3, batch_norm=False, dilation=False):\n    layers = []\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            padding = 2 if dilation else 1\n            kernel_size = 3\n            dilation_rate = 2 if dilation else 1\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=kernel_size, padding=padding, dilation=dilation_rate)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\n# -------------------------\n# Utilities: saving, plotting, evaluation\n# -------------------------\ndef ensure_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)\n\ndef save_training_csv(metrics, out_path):\n    df = pd.DataFrame(metrics)\n    df.to_csv(out_path, index=False)\n\ndef save_model_architecture_csv(model, out_csv, dummy_input_shape=(1,3,512,512)):\n    rows = []\n    # add layer names and param counts\n    for name, module in model.named_modules():\n        if name == '':\n            continue\n        param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)\n        rows.append({'layer_name': name, 'layer_type': module.__class__.__name__, 'param_count': param_count})\n\n    # Optional: try to get output shapes by running a dummy forward (best-effort)\n    try:\n        x = torch.randn(dummy_input_shape)\n        x = x.to(next(model.parameters()).device)\n        outputs = []\n        hooks = []\n        def make_hook(name):\n            def hook(module, inp, out):\n                try:\n                    outputs.append((name, tuple(out.shape) if isinstance(out, torch.Tensor) else str(type(out))))\n                except Exception:\n                    outputs.append((name, 'unknown'))\n            return hook\n        for name, module in model.named_modules():\n            if len(list(module.children())) == 0:\n                hooks.append(module.register_forward_hook(make_hook(name)))\n        model.eval()\n        with torch.no_grad():\n            _ = model(x)\n        for h in hooks:\n            h.remove()\n        # merge output shapes into rows\n        shape_map = {n:s for n,s in outputs}\n        for r in rows:\n            r['output_shape'] = shape_map.get(r['layer_name'], '')\n    except Exception:\n        # ignore shape extraction failure\n        for r in rows:\n            r['output_shape'] = ''\n\n    df = pd.DataFrame(rows)\n    df.to_csv(out_csv, index=False)\n\ndef plot_training_loss(metrics_list, out_png):\n    epochs = [m['epoch'] for m in metrics_list]\n    train_loss = [m['train_loss'] for m in metrics_list]\n    val_mae = [m['val_MAE'] for m in metrics_list]\n    val_rmse = [m['val_RMSE'] for m in metrics_list]\n\n    plt.figure(figsize=(8,6))\n    plt.plot(epochs, train_loss, marker='o', label='train_loss')\n    plt.plot(epochs, val_mae, marker='o', label='val_MAE')\n    plt.plot(epochs, val_rmse, marker='o', label='val_RMSE')\n    plt.xlabel('Epoch')\n    plt.ylabel('Value')\n    plt.title('CSRNet Training Metrics')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\ndef evaluate_model_on_loader(model, loader, device, max_samples=None, return_all_preds=False):\n    model.eval()\n    preds = []\n    gts = []\n    img_paths = []\n    with torch.no_grad():\n        for i, (img, target) in enumerate(loader):\n            if max_samples is not None and i >= max_samples:\n                break\n            img = img.to(device)\n            target = target.to(device)\n\n            output = model(img)\n            pred_count = float(output.detach().cpu().sum().item())\n            gt_count = float(target.detach().cpu().sum().item())\n\n            preds.append(pred_count)\n            gts.append(gt_count)\n            if hasattr(loader.dataset, 'img_paths'):\n                try:\n                    img_paths.append(loader.dataset.img_paths[i])\n                except Exception:\n                    img_paths.append(None)\n            else:\n                img_paths.append(None)\n\n    preds = np.array(preds)\n    gts = np.array(gts)\n    abs_errors = np.abs(preds - gts)\n    mae = float(np.mean(abs_errors)) if len(abs_errors) > 0 else float('nan')\n    rmse = float(np.sqrt(np.mean((preds - gts)**2))) if len(abs_errors) > 0 else float('nan')\n\n    if return_all_preds:\n        return {'preds': preds, 'gts': gts, 'img_paths': img_paths, 'mae': mae, 'rmse': rmse}\n    else:\n        return mae, rmse\n\ndef save_predictions_csv(img_paths, preds, gts, out_csv):\n    rows = []\n    for p, pr, gt in zip(img_paths, preds, gts):\n        rows.append({'img_path': p, 'pred_count': float(pr), 'gt_count': float(gt), 'abs_error': float(abs(pr-gt))})\n    df = pd.DataFrame(rows)\n    df.to_csv(out_csv, index=False)\n\ndef plot_gt_vs_pred(gts, preds, out_png, title=\"GT vs Pred\"):\n    plt.figure(figsize=(6,6))\n    plt.scatter(gts, preds, alpha=0.6)\n    mn = min(np.min(gts), np.min(preds)) if len(gts)>0 and len(preds)>0 else 0\n    mx = max(np.max(gts), np.max(preds)) if len(gts)>0 and len(preds)>0 else 1\n    plt.plot([mn, mx], [mn, mx], 'r--', linewidth=1)\n    plt.xlabel('Ground Truth Count')\n    plt.ylabel('Predicted Count')\n    plt.title(title)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\ndef plot_residuals(gts, preds, out_png):\n    residuals = preds - gts\n    plt.figure(figsize=(8,5))\n    plt.scatter(gts, residuals, alpha=0.6)\n    plt.axhline(0, color='r', linestyle='--')\n    plt.xlabel('Ground Truth Count')\n    plt.ylabel('Residual (Pred - GT)')\n    plt.title('Residuals vs GT')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\ndef plot_error_histogram(errors, out_png):\n    plt.figure(figsize=(6,4))\n    plt.hist(errors, bins=30)\n    plt.xlabel('Absolute Error')\n    plt.ylabel('Frequency')\n    plt.title('Error Histogram')\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\n\n# -------------------------\n# Training pipeline\n# -------------------------\ndef train_csrnet():\n    DATA_ROOT = \"/kaggle/input/shanghaitech/ShanghaiTech\"\n    OUT_ROOT = \"/kaggle/working/csr_net\"\n    ensure_dir(OUT_ROOT)\n\n    model_dir = os.path.join(OUT_ROOT, \"models\")\n    plots_dir = os.path.join(OUT_ROOT, \"plots\")\n    csv_dir = os.path.join(OUT_ROOT, \"csv\")\n    ensure_dir(model_dir); ensure_dir(plots_dir); ensure_dir(csv_dir)\n\n    # Check if dataset exists\n    if not os.path.exists(DATA_ROOT):\n        print(f\"Dataset not found at {DATA_ROOT}\")\n        return\n\n    # Hyperparameters\n    LR = 1e-5\n    BATCH_SIZE = 4\n    EPOCHS = 30\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    print(f\"Using device: {DEVICE}\")\n\n    # Datasets\n    train_ds = CSRNetDataset(DATA_ROOT, part='A', mode='train')\n    val_ds = CSRNetDataset(DATA_ROOT, part='A', mode='test')\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0)\n\n    # Model\n    model = CSRNet(load_weights=True).to(DEVICE)\n\n    # Save model architecture CSV right away (includes best-effort output shapes)\n    save_model_architecture_csv(model, os.path.join(csv_dir, \"model_architecture.csv\"))\n\n    criterion = nn.MSELoss(reduction='sum')\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n    best_mae = float('inf')\n    best_epoch = -1\n    metrics_history = []\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss_sum = 0.0\n        batch_count = 0\n\n        for i, (img, target) in enumerate(train_loader):\n            img = img.to(DEVICE)\n            target = target.to(DEVICE)\n\n            output = model(img)\n            loss = criterion(output, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss_sum += float(loss.item())\n            batch_count += 1\n\n            if i % 10 == 0:\n                print(f\"Epoch {epoch+1}/{EPOCHS}, Step {i}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n\n        train_loss_avg = train_loss_sum / max(1, batch_count)\n\n        # Validation\n        model.eval()\n        mae = 0.0\n        mse_sum = 0.0\n        val_samples = 0\n\n        with torch.no_grad():\n            for img, target in val_loader:\n                img = img.to(DEVICE)\n                target = target.to(DEVICE)\n                output = model(img)\n\n                pred_count = float(output.detach().cpu().sum().item())\n                gt_count = float(target.detach().cpu().sum().item())\n\n                mae += abs(pred_count - gt_count)\n                mse_sum += (pred_count - gt_count) ** 2\n                val_samples += 1\n\n        val_mae = float(mae / val_samples) if val_samples > 0 else float('nan')\n        val_rmse = float(math.sqrt(mse_sum / val_samples)) if val_samples > 0 else float('nan')\n\n        print(f\"Epoch {epoch+1} Result: MAE: {val_mae:.2f}, RMSE: {val_rmse:.2f}, train_loss_avg: {train_loss_avg:.4f}\")\n\n        metrics_history.append({'epoch': epoch+1, 'train_loss': train_loss_avg, 'val_MAE': val_mae, 'val_RMSE': val_rmse})\n\n        # Save training CSV each epoch (overwrites but keeps latest history)\n        save_training_csv(metrics_history, os.path.join(csv_dir, \"csr_training_metrics.csv\"))\n\n        # Save best model (by MAE)\n        if val_mae < best_mae:\n            best_mae = val_mae\n            best_epoch = epoch + 1\n            best_model_path = os.path.join(model_dir, \"best_csrnet_model.pth\")\n            torch.save(model.state_dict(), best_model_path)\n            pd.DataFrame([{'best_epoch': best_epoch, 'best_MAE': best_mae, 'best_RMSE': val_rmse, 'saved_at': datetime.now().isoformat()}]).to_csv(os.path.join(csv_dir, \"best_model_metrics.csv\"), index=False)\n            print(f\"Saved best model at epoch {best_epoch} with MAE {best_mae:.4f}\")\n\n    # End training: save final full model\n    final_model_path = os.path.join(model_dir, \"final_csrnet_model.pth\")\n    torch.save(model.state_dict(), final_model_path)\n    print(\"Saved final model:\", final_model_path)\n\n    # Plot training metrics\n    plot_training_loss(metrics_history, os.path.join(plots_dir, \"csr_training_plot.png\"))\n\n    # Load best model for further evaluation & predictions\n    best_model_file = os.path.join(model_dir, \"best_csrnet_model.pth\")\n    if not os.path.exists(best_model_file):\n        print(\"Best model file not found, using final model for evaluation\")\n        best_model_file = final_model_path\n\n    best_model = CSRNet(load_weights=False).to(DEVICE)\n    best_model.load_state_dict(torch.load(best_model_file, map_location=DEVICE))\n    best_model.eval()\n\n    # Evaluate best model on full validation set\n    print(\"Evaluating best model on full validation set...\")\n    full_val_eval = evaluate_model_on_loader(best_model, val_loader, DEVICE, max_samples=None, return_all_preds=True)\n    final_metrics_df = pd.DataFrame([{'MAE': full_val_eval['mae'], 'RMSE': full_val_eval['rmse']}])\n    final_metrics_df.to_csv(os.path.join(csv_dir, \"final_error_matrix_best_model.csv\"), index=False)\n\n    # Save predictions on samples for plots\n    SAMPLE_N = 200\n    print(f\"Generating predictions on up to {SAMPLE_N} validation samples and {SAMPLE_N} train samples for plots...\")\n    val_sample_eval = evaluate_model_on_loader(best_model, val_loader, DEVICE, max_samples=SAMPLE_N, return_all_preds=True)\n    train_sample_eval = evaluate_model_on_loader(best_model, train_loader, DEVICE, max_samples=SAMPLE_N, return_all_preds=True)\n\n    sample_preds = val_sample_eval['preds']\n    sample_gts = val_sample_eval['gts']\n    sample_paths = val_sample_eval['img_paths']\n\n    save_predictions_csv(sample_paths, sample_preds, sample_gts, os.path.join(csv_dir, \"predictions_val_sample.csv\"))\n\n    # Plots: GT vs Pred for sample\n    if len(sample_gts) > 0:\n        plot_gt_vs_pred(sample_gts, sample_preds, os.path.join(plots_dir, \"csr_gt_vs_pred.png\"), title=\"GT vs Pred (validation sample)\")\n        n50 = min(50, len(sample_gts))\n        plot_gt_vs_pred(sample_gts[:n50], sample_preds[:n50], os.path.join(plots_dir, \"csr_gt_vs_pred_first50.png\"), title=\"GT vs Pred (first 50 val samples)\")\n        plot_residuals(sample_gts, sample_preds, os.path.join(plots_dir, \"csr_residual_plot.png\"))\n        plot_error_histogram(np.abs(sample_preds - sample_gts), os.path.join(plots_dir, \"csr_error_histogram.png\"))\n\n    # Combined CSV for train and val sample comparison\n    combined_rows = []\n    for split_name, e in [('val', val_sample_eval), ('train', train_sample_eval)]:\n        for p, pr, gt in zip(e['img_paths'], e['preds'], e['gts']):\n            combined_rows.append({'split': split_name, 'img_path': p, 'pred_count': float(pr), 'gt_count': float(gt), 'abs_error': float(abs(pr-gt))})\n    pd.DataFrame(combined_rows).to_csv(os.path.join(csv_dir, \"predictions_combined_sample.csv\"), index=False)\n\n    print(\"All artifacts saved under:\", OUT_ROOT)\n    print(\" - Models: \", model_dir)\n    print(\" - CSVs: \", csv_dir)\n    print(\" - Plots: \", plots_dir)\n    print(\"Training finished. Best epoch:\", best_epoch, \"Best MAE:\", best_mae)\n\n\nif __name__ == \"__main__\":\n    train_csrnet()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:03:05.363521Z","iopub.execute_input":"2025-12-05T10:03:05.364303Z","iopub.status.idle":"2025-12-05T10:32:52.022418Z","shell.execute_reply.started":"2025-12-05T10:03:05.364277Z","shell.execute_reply":"2025-12-05T10:32:52.021733Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 223MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30, Step 0/75, Loss: 522.3301\nEpoch 1/30, Step 10/75, Loss: 496.6983\nEpoch 1/30, Step 20/75, Loss: 217.6971\nEpoch 1/30, Step 30/75, Loss: 766.0208\nEpoch 1/30, Step 40/75, Loss: 158.6265\nEpoch 1/30, Step 50/75, Loss: 125.3414\nEpoch 1/30, Step 60/75, Loss: 90.4042\nEpoch 1/30, Step 70/75, Loss: 115.6695\nEpoch 1 Result: MAE: 104.38, RMSE: 147.70, train_loss_avg: 288.5111\nSaved best model at epoch 1 with MAE 104.3764\nEpoch 2/30, Step 0/75, Loss: 16.3905\nEpoch 2/30, Step 10/75, Loss: 82.7026\nEpoch 2/30, Step 20/75, Loss: 45.7523\nEpoch 2/30, Step 30/75, Loss: 311.2673\nEpoch 2/30, Step 40/75, Loss: 49.6816\nEpoch 2/30, Step 50/75, Loss: 71.5258\nEpoch 2/30, Step 60/75, Loss: 107.3979\nEpoch 2/30, Step 70/75, Loss: 115.6520\nEpoch 2 Result: MAE: 80.31, RMSE: 111.06, train_loss_avg: 83.6308\nSaved best model at epoch 2 with MAE 80.3089\nEpoch 3/30, Step 0/75, Loss: 53.8095\nEpoch 3/30, Step 10/75, Loss: 58.9959\nEpoch 3/30, Step 20/75, Loss: 61.3067\nEpoch 3/30, Step 30/75, Loss: 28.2730\nEpoch 3/30, Step 40/75, Loss: 25.2377\nEpoch 3/30, Step 50/75, Loss: 112.5429\nEpoch 3/30, Step 60/75, Loss: 26.2115\nEpoch 3/30, Step 70/75, Loss: 118.9332\nEpoch 3 Result: MAE: 95.55, RMSE: 129.83, train_loss_avg: 57.5800\nEpoch 4/30, Step 0/75, Loss: 59.2414\nEpoch 4/30, Step 10/75, Loss: 42.5380\nEpoch 4/30, Step 20/75, Loss: 26.7056\nEpoch 4/30, Step 30/75, Loss: 70.9041\nEpoch 4/30, Step 40/75, Loss: 90.8917\nEpoch 4/30, Step 50/75, Loss: 58.9600\nEpoch 4/30, Step 60/75, Loss: 52.5127\nEpoch 4/30, Step 70/75, Loss: 105.5134\nEpoch 4 Result: MAE: 71.91, RMSE: 99.50, train_loss_avg: 52.0636\nSaved best model at epoch 4 with MAE 71.9083\nEpoch 5/30, Step 0/75, Loss: 21.4225\nEpoch 5/30, Step 10/75, Loss: 29.7626\nEpoch 5/30, Step 20/75, Loss: 21.4211\nEpoch 5/30, Step 30/75, Loss: 37.0138\nEpoch 5/30, Step 40/75, Loss: 17.9988\nEpoch 5/30, Step 50/75, Loss: 88.0961\nEpoch 5/30, Step 60/75, Loss: 21.9295\nEpoch 5/30, Step 70/75, Loss: 24.5778\nEpoch 5 Result: MAE: 63.89, RMSE: 89.12, train_loss_avg: 47.2766\nSaved best model at epoch 5 with MAE 63.8909\nEpoch 6/30, Step 0/75, Loss: 22.5761\nEpoch 6/30, Step 10/75, Loss: 11.2506\nEpoch 6/30, Step 20/75, Loss: 75.6798\nEpoch 6/30, Step 30/75, Loss: 21.7972\nEpoch 6/30, Step 40/75, Loss: 17.7428\nEpoch 6/30, Step 50/75, Loss: 102.6002\nEpoch 6/30, Step 60/75, Loss: 43.7899\nEpoch 6/30, Step 70/75, Loss: 22.7849\nEpoch 6 Result: MAE: 64.20, RMSE: 86.30, train_loss_avg: 44.2178\nEpoch 7/30, Step 0/75, Loss: 48.8622\nEpoch 7/30, Step 10/75, Loss: 62.5219\nEpoch 7/30, Step 20/75, Loss: 35.9025\nEpoch 7/30, Step 30/75, Loss: 58.9021\nEpoch 7/30, Step 40/75, Loss: 19.1831\nEpoch 7/30, Step 50/75, Loss: 13.4727\nEpoch 7/30, Step 60/75, Loss: 12.0193\nEpoch 7/30, Step 70/75, Loss: 62.2336\nEpoch 7 Result: MAE: 71.90, RMSE: 98.22, train_loss_avg: 37.9553\nEpoch 8/30, Step 0/75, Loss: 34.4379\nEpoch 8/30, Step 10/75, Loss: 30.5636\nEpoch 8/30, Step 20/75, Loss: 26.3330\nEpoch 8/30, Step 30/75, Loss: 27.5237\nEpoch 8/30, Step 40/75, Loss: 25.0352\nEpoch 8/30, Step 50/75, Loss: 31.1332\nEpoch 8/30, Step 60/75, Loss: 31.2392\nEpoch 8/30, Step 70/75, Loss: 34.7624\nEpoch 8 Result: MAE: 71.07, RMSE: 96.53, train_loss_avg: 36.8555\nEpoch 9/30, Step 0/75, Loss: 68.4983\nEpoch 9/30, Step 10/75, Loss: 30.1170\nEpoch 9/30, Step 20/75, Loss: 16.7847\nEpoch 9/30, Step 30/75, Loss: 18.4548\nEpoch 9/30, Step 40/75, Loss: 39.7401\nEpoch 9/30, Step 50/75, Loss: 25.2243\nEpoch 9/30, Step 60/75, Loss: 63.1287\nEpoch 9/30, Step 70/75, Loss: 22.4436\nEpoch 9 Result: MAE: 99.30, RMSE: 124.08, train_loss_avg: 38.6610\nEpoch 10/30, Step 0/75, Loss: 31.7551\nEpoch 10/30, Step 10/75, Loss: 20.2767\nEpoch 10/30, Step 20/75, Loss: 43.4007\nEpoch 10/30, Step 30/75, Loss: 18.0164\nEpoch 10/30, Step 40/75, Loss: 33.8028\nEpoch 10/30, Step 50/75, Loss: 36.0416\nEpoch 10/30, Step 60/75, Loss: 71.6578\nEpoch 10/30, Step 70/75, Loss: 24.4537\nEpoch 10 Result: MAE: 74.09, RMSE: 103.49, train_loss_avg: 38.1937\nEpoch 11/30, Step 0/75, Loss: 22.6767\nEpoch 11/30, Step 10/75, Loss: 31.2146\nEpoch 11/30, Step 20/75, Loss: 46.7695\nEpoch 11/30, Step 30/75, Loss: 31.1802\nEpoch 11/30, Step 40/75, Loss: 40.4690\nEpoch 11/30, Step 50/75, Loss: 45.7567\nEpoch 11/30, Step 60/75, Loss: 25.6221\nEpoch 11/30, Step 70/75, Loss: 16.0501\nEpoch 11 Result: MAE: 62.14, RMSE: 85.75, train_loss_avg: 35.6366\nSaved best model at epoch 11 with MAE 62.1360\nEpoch 12/30, Step 0/75, Loss: 36.9172\nEpoch 12/30, Step 10/75, Loss: 40.0717\nEpoch 12/30, Step 20/75, Loss: 46.8703\nEpoch 12/30, Step 30/75, Loss: 12.9872\nEpoch 12/30, Step 40/75, Loss: 11.4342\nEpoch 12/30, Step 50/75, Loss: 18.6318\nEpoch 12/30, Step 60/75, Loss: 24.9911\nEpoch 12/30, Step 70/75, Loss: 15.0291\nEpoch 12 Result: MAE: 55.02, RMSE: 74.64, train_loss_avg: 31.0315\nSaved best model at epoch 12 with MAE 55.0250\nEpoch 13/30, Step 0/75, Loss: 39.6045\nEpoch 13/30, Step 10/75, Loss: 41.3540\nEpoch 13/30, Step 20/75, Loss: 18.2518\nEpoch 13/30, Step 30/75, Loss: 27.7955\nEpoch 13/30, Step 40/75, Loss: 13.2135\nEpoch 13/30, Step 50/75, Loss: 23.6636\nEpoch 13/30, Step 60/75, Loss: 25.9732\nEpoch 13/30, Step 70/75, Loss: 28.1382\nEpoch 13 Result: MAE: 69.18, RMSE: 95.40, train_loss_avg: 28.1150\nEpoch 14/30, Step 0/75, Loss: 36.1967\nEpoch 14/30, Step 10/75, Loss: 20.3941\nEpoch 14/30, Step 20/75, Loss: 20.2276\nEpoch 14/30, Step 30/75, Loss: 19.5851\nEpoch 14/30, Step 40/75, Loss: 16.1862\nEpoch 14/30, Step 50/75, Loss: 18.2052\nEpoch 14/30, Step 60/75, Loss: 9.3110\nEpoch 14/30, Step 70/75, Loss: 46.3727\nEpoch 14 Result: MAE: 58.49, RMSE: 80.35, train_loss_avg: 27.4834\nEpoch 15/30, Step 0/75, Loss: 19.1435\nEpoch 15/30, Step 10/75, Loss: 19.0817\nEpoch 15/30, Step 20/75, Loss: 12.9542\nEpoch 15/30, Step 30/75, Loss: 34.5570\nEpoch 15/30, Step 40/75, Loss: 61.0596\nEpoch 15/30, Step 50/75, Loss: 24.1259\nEpoch 15/30, Step 60/75, Loss: 10.2964\nEpoch 15/30, Step 70/75, Loss: 22.0658\nEpoch 15 Result: MAE: 55.59, RMSE: 80.31, train_loss_avg: 27.3005\nEpoch 16/30, Step 0/75, Loss: 63.2874\nEpoch 16/30, Step 10/75, Loss: 17.9199\nEpoch 16/30, Step 20/75, Loss: 12.0499\nEpoch 16/30, Step 30/75, Loss: 19.7733\nEpoch 16/30, Step 40/75, Loss: 22.1239\nEpoch 16/30, Step 50/75, Loss: 25.2278\nEpoch 16/30, Step 60/75, Loss: 18.8376\nEpoch 16/30, Step 70/75, Loss: 15.4948\nEpoch 16 Result: MAE: 62.98, RMSE: 87.54, train_loss_avg: 26.3430\nEpoch 17/30, Step 0/75, Loss: 29.7366\nEpoch 17/30, Step 10/75, Loss: 19.1638\nEpoch 17/30, Step 20/75, Loss: 24.8486\nEpoch 17/30, Step 30/75, Loss: 19.8440\nEpoch 17/30, Step 40/75, Loss: 23.7205\nEpoch 17/30, Step 50/75, Loss: 15.6123\nEpoch 17/30, Step 60/75, Loss: 18.8454\nEpoch 17/30, Step 70/75, Loss: 40.2192\nEpoch 17 Result: MAE: 61.48, RMSE: 81.24, train_loss_avg: 30.3936\nEpoch 18/30, Step 0/75, Loss: 33.0874\nEpoch 18/30, Step 10/75, Loss: 90.1139\nEpoch 18/30, Step 20/75, Loss: 21.2564\nEpoch 18/30, Step 30/75, Loss: 12.7195\nEpoch 18/30, Step 40/75, Loss: 22.0145\nEpoch 18/30, Step 50/75, Loss: 15.5749\nEpoch 18/30, Step 60/75, Loss: 30.1119\nEpoch 18/30, Step 70/75, Loss: 18.6053\nEpoch 18 Result: MAE: 55.09, RMSE: 80.49, train_loss_avg: 29.2027\nEpoch 19/30, Step 0/75, Loss: 16.1424\nEpoch 19/30, Step 10/75, Loss: 10.3557\nEpoch 19/30, Step 20/75, Loss: 21.2983\nEpoch 19/30, Step 30/75, Loss: 13.0008\nEpoch 19/30, Step 40/75, Loss: 44.6523\nEpoch 19/30, Step 50/75, Loss: 10.4168\nEpoch 19/30, Step 60/75, Loss: 24.2258\nEpoch 19/30, Step 70/75, Loss: 28.6290\nEpoch 19 Result: MAE: 60.45, RMSE: 82.75, train_loss_avg: 27.2383\nEpoch 20/30, Step 0/75, Loss: 33.4854\nEpoch 20/30, Step 10/75, Loss: 14.9514\nEpoch 20/30, Step 20/75, Loss: 66.1367\nEpoch 20/30, Step 30/75, Loss: 26.4743\nEpoch 20/30, Step 40/75, Loss: 20.5702\nEpoch 20/30, Step 50/75, Loss: 23.8356\nEpoch 20/30, Step 60/75, Loss: 28.0536\nEpoch 20/30, Step 70/75, Loss: 21.2177\nEpoch 20 Result: MAE: 80.90, RMSE: 105.27, train_loss_avg: 25.0919\nEpoch 21/30, Step 0/75, Loss: 23.0831\nEpoch 21/30, Step 10/75, Loss: 35.6258\nEpoch 21/30, Step 20/75, Loss: 19.8422\nEpoch 21/30, Step 30/75, Loss: 20.6112\nEpoch 21/30, Step 40/75, Loss: 26.2797\nEpoch 21/30, Step 50/75, Loss: 22.4516\nEpoch 21/30, Step 60/75, Loss: 15.8548\nEpoch 21/30, Step 70/75, Loss: 23.6293\nEpoch 21 Result: MAE: 52.16, RMSE: 74.09, train_loss_avg: 22.3198\nSaved best model at epoch 21 with MAE 52.1574\nEpoch 22/30, Step 0/75, Loss: 14.9868\nEpoch 22/30, Step 10/75, Loss: 22.3951\nEpoch 22/30, Step 20/75, Loss: 22.4434\nEpoch 22/30, Step 30/75, Loss: 23.6468\nEpoch 22/30, Step 40/75, Loss: 30.2265\nEpoch 22/30, Step 50/75, Loss: 19.0122\nEpoch 22/30, Step 60/75, Loss: 15.2601\nEpoch 22/30, Step 70/75, Loss: 34.8853\nEpoch 22 Result: MAE: 53.90, RMSE: 77.70, train_loss_avg: 21.1169\nEpoch 23/30, Step 0/75, Loss: 24.5679\nEpoch 23/30, Step 10/75, Loss: 15.0404\nEpoch 23/30, Step 20/75, Loss: 42.6722\nEpoch 23/30, Step 30/75, Loss: 17.8039\nEpoch 23/30, Step 40/75, Loss: 14.1607\nEpoch 23/30, Step 50/75, Loss: 6.9888\nEpoch 23/30, Step 60/75, Loss: 28.1826\nEpoch 23/30, Step 70/75, Loss: 10.9070\nEpoch 23 Result: MAE: 50.40, RMSE: 73.67, train_loss_avg: 21.6229\nSaved best model at epoch 23 with MAE 50.4007\nEpoch 24/30, Step 0/75, Loss: 16.1405\nEpoch 24/30, Step 10/75, Loss: 12.1830\nEpoch 24/30, Step 20/75, Loss: 15.0345\nEpoch 24/30, Step 30/75, Loss: 12.9264\nEpoch 24/30, Step 40/75, Loss: 24.0124\nEpoch 24/30, Step 50/75, Loss: 20.1435\nEpoch 24/30, Step 60/75, Loss: 16.3628\nEpoch 24/30, Step 70/75, Loss: 9.8985\nEpoch 24 Result: MAE: 51.42, RMSE: 72.99, train_loss_avg: 22.1957\nEpoch 25/30, Step 0/75, Loss: 25.2792\nEpoch 25/30, Step 10/75, Loss: 11.5394\nEpoch 25/30, Step 20/75, Loss: 20.2700\nEpoch 25/30, Step 30/75, Loss: 18.6912\nEpoch 25/30, Step 40/75, Loss: 35.1928\nEpoch 25/30, Step 50/75, Loss: 35.2421\nEpoch 25/30, Step 60/75, Loss: 16.1641\nEpoch 25/30, Step 70/75, Loss: 18.7212\nEpoch 25 Result: MAE: 59.73, RMSE: 86.45, train_loss_avg: 24.3502\nEpoch 26/30, Step 0/75, Loss: 18.8542\nEpoch 26/30, Step 10/75, Loss: 22.8552\nEpoch 26/30, Step 20/75, Loss: 19.0749\nEpoch 26/30, Step 30/75, Loss: 37.9992\nEpoch 26/30, Step 40/75, Loss: 22.7848\nEpoch 26/30, Step 50/75, Loss: 20.1458\nEpoch 26/30, Step 60/75, Loss: 16.2736\nEpoch 26/30, Step 70/75, Loss: 18.2568\nEpoch 26 Result: MAE: 55.48, RMSE: 78.62, train_loss_avg: 21.1565\nEpoch 27/30, Step 0/75, Loss: 16.4489\nEpoch 27/30, Step 10/75, Loss: 19.7868\nEpoch 27/30, Step 20/75, Loss: 17.4087\nEpoch 27/30, Step 30/75, Loss: 12.4619\nEpoch 27/30, Step 40/75, Loss: 18.6109\nEpoch 27/30, Step 50/75, Loss: 24.4747\nEpoch 27/30, Step 60/75, Loss: 18.5881\nEpoch 27/30, Step 70/75, Loss: 12.2399\nEpoch 27 Result: MAE: 55.68, RMSE: 78.78, train_loss_avg: 19.3815\nEpoch 28/30, Step 0/75, Loss: 14.7273\nEpoch 28/30, Step 10/75, Loss: 21.5780\nEpoch 28/30, Step 20/75, Loss: 12.4721\nEpoch 28/30, Step 30/75, Loss: 21.6513\nEpoch 28/30, Step 40/75, Loss: 33.0632\nEpoch 28/30, Step 50/75, Loss: 12.0515\nEpoch 28/30, Step 60/75, Loss: 16.4085\nEpoch 28/30, Step 70/75, Loss: 24.8840\nEpoch 28 Result: MAE: 49.27, RMSE: 72.03, train_loss_avg: 18.1847\nSaved best model at epoch 28 with MAE 49.2672\nEpoch 29/30, Step 0/75, Loss: 15.3430\nEpoch 29/30, Step 10/75, Loss: 24.7463\nEpoch 29/30, Step 20/75, Loss: 9.3896\nEpoch 29/30, Step 30/75, Loss: 18.0798\nEpoch 29/30, Step 40/75, Loss: 25.6042\nEpoch 29/30, Step 50/75, Loss: 12.7293\nEpoch 29/30, Step 60/75, Loss: 19.0193\nEpoch 29/30, Step 70/75, Loss: 16.5032\nEpoch 29 Result: MAE: 51.23, RMSE: 74.22, train_loss_avg: 18.8574\nEpoch 30/30, Step 0/75, Loss: 20.1024\nEpoch 30/30, Step 10/75, Loss: 9.5681\nEpoch 30/30, Step 20/75, Loss: 12.8036\nEpoch 30/30, Step 30/75, Loss: 31.7843\nEpoch 30/30, Step 40/75, Loss: 37.9802\nEpoch 30/30, Step 50/75, Loss: 10.5575\nEpoch 30/30, Step 60/75, Loss: 30.2757\nEpoch 30/30, Step 70/75, Loss: 12.7115\nEpoch 30 Result: MAE: 57.20, RMSE: 82.23, train_loss_avg: 19.1315\nSaved final model: /kaggle/working/csr_net/models/final_csrnet_model.pth\nEvaluating best model on full validation set...\nGenerating predictions on up to 200 validation samples and 200 train samples for plots...\nAll artifacts saved under: /kaggle/working/csr_net\n - Models:  /kaggle/working/csr_net/models\n - CSVs:  /kaggle/working/csr_net/csv\n - Plots:  /kaggle/working/csr_net/plots\nTraining finished. Best epoch: 28 Best MAE: 49.26724010509449\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### MobileNet-CSRNet Model","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom scipy.ndimage import gaussian_filter\nfrom PIL import Image\nfrom scipy.io import loadmat\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nfrom datetime import datetime\n\n\n# -------------------------\n# Density Map Generation\n# -------------------------\ndef generate_density_map(shape, points, sigma=1.0):\n    \"\"\"\n    Generate a density map for a given shape and points.\n    shape: (H, W) of the output density map\n    points: [(x, y), ...] coordinates on the output density map scale\n    sigma: Gaussian sigma\n    \"\"\"\n    H, W = shape\n    density = np.zeros((H, W), dtype=np.float32)\n\n    # Filter points that are out of bounds\n    for p in points:\n        x, y = int(p[0]), int(p[1])\n        if 0 <= x < W and 0 <= y < H:\n            density[y, x] += 1.0\n\n    density = gaussian_filter(density, sigma=sigma, mode='constant', cval=0)\n    return density\n\n\n# -------------------------\n# Dataset\n# -------------------------\nclass CSRNetDataset(Dataset):\n    def __init__(self, root_dir, part='A', mode='train', target_size=(512, 512), downsample_ratio=8):\n        self.img_dir = os.path.join(root_dir, f\"part_{part}\", f\"{mode}_data\", \"images\")\n        self.gt_dir = os.path.join(root_dir, f\"part_{part}\", f\"{mode}_data\", \"ground-truth\")\n\n        self.img_paths = sorted(glob.glob(os.path.join(self.img_dir, \"*.jpg\")))\n        self.target_size = target_size\n        self.downsample_ratio = downsample_ratio\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        img = Image.open(img_path).convert('RGB')\n        orig_w, orig_h = img.size\n\n        img_resized = img.resize(self.target_size[::-1], Image.BILINEAR)\n\n        filename = os.path.basename(img_path).replace('.jpg', '')\n        gt_path = os.path.join(self.gt_dir, f\"GT_{filename}.mat\")\n\n        if os.path.exists(gt_path):\n            mat = loadmat(gt_path)\n            try:\n                points = mat['image_info'][0][0][0][0][0]\n            except Exception:\n                points = np.array([])\n        else:\n            points = np.array([])\n\n        scale_x = self.target_size[1] / float(orig_w)\n        scale_y = self.target_size[0] / float(orig_h)\n\n        points_resized = points.copy()\n        if len(points_resized) > 0:\n            points_resized = points_resized.astype(np.float32)\n            points_resized[:, 0] *= scale_x\n            points_resized[:, 1] *= scale_y\n\n        out_h = self.target_size[0] // self.downsample_ratio\n        out_w = self.target_size[1] // self.downsample_ratio\n\n        points_map = points_resized.copy()\n        if len(points_map) > 0:\n            points_map = points_map / float(self.downsample_ratio)\n\n        density = generate_density_map((out_h, out_w), points_map, sigma=2.0)\n\n        img_tensor = self.transform(img_resized)\n        density_tensor = torch.from_numpy(density).unsqueeze(0)\n\n        return img_tensor, density_tensor\n\n\n# -------------------------\n# MobileNet-based CSRNet model\n# -------------------------\nclass MobileNetCSRNet(nn.Module):\n    def __init__(self, load_weights=True):\n        super(MobileNetCSRNet, self).__init__()\n        weights = models.MobileNet_V2_Weights.DEFAULT if load_weights else None\n        mn = models.mobilenet_v2(weights=weights)\n\n        # frontend: first 7 feature layers give roughly 1/8 spatial scale\n        self.frontend = nn.Sequential(*list(mn.features.children())[:7])\n\n        # backend: projection to higher channels then dilated convs\n        self.backend = nn.Sequential(\n            nn.Conv2d(32, 512, kernel_size=3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 256, kernel_size=3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 128, kernel_size=3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 64, kernel_size=3, padding=2, dilation=2),\n            nn.ReLU(inplace=True)\n        )\n\n        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.frontend(x)\n        x = self.backend(x)\n        x = self.output_layer(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.backend.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        nn.init.normal_(self.output_layer.weight, std=0.01)\n        nn.init.constant_(self.output_layer.bias, 0)\n\n\n# -------------------------\n# Utilities: saving, plotting, evaluation\n# -------------------------\ndef ensure_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)\n\n\ndef save_training_csv(metrics, out_path):\n    df = pd.DataFrame(metrics)\n    df.to_csv(out_path, index=False)\n\n\ndef save_model_architecture_csv(model, out_csv, dummy_input_shape=(1,3,512,512), device='cpu'):\n    rows = []\n    for name, module in model.named_modules():\n        if name == '':\n            continue\n        param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)\n        rows.append({'layer_name': name, 'layer_type': module.__class__.__name__, 'param_count': param_count})\n\n    # Try to get output shapes using hooks\n    try:\n        model = model.to(device)\n        x = torch.randn(dummy_input_shape).to(device)\n        outputs = []\n        hooks = []\n        def make_hook(name):\n            def hook(module, inp, out):\n                try:\n                    if isinstance(out, torch.Tensor):\n                        outputs.append((name, tuple(out.shape)))\n                    else:\n                        outputs.append((name, str(type(out))))\n                except Exception:\n                    outputs.append((name, 'unknown'))\n            return hook\n\n        for name, module in model.named_modules():\n            if len(list(module.children())) == 0:\n                hooks.append(module.register_forward_hook(make_hook(name)))\n\n        model.eval()\n        with torch.no_grad():\n            _ = model(x)\n        for h in hooks:\n            h.remove()\n\n        shape_map = {n: s for n, s in outputs}\n        for r in rows:\n            r['output_shape'] = shape_map.get(r['layer_name'], '')\n    except Exception:\n        for r in rows:\n            r['output_shape'] = ''\n\n    df = pd.DataFrame(rows)\n    df.to_csv(out_csv, index=False)\n\n\ndef plot_training_loss(metrics_list, out_png):\n    epochs = [m['epoch'] for m in metrics_list]\n    train_loss = [m['train_loss'] for m in metrics_list]\n    val_mae = [m['val_MAE'] for m in metrics_list]\n    val_rmse = [m['val_RMSE'] for m in metrics_list]\n\n    plt.figure(figsize=(8,6))\n    plt.plot(epochs, train_loss, marker='o', label='train_loss')\n    plt.plot(epochs, val_mae, marker='o', label='val_MAE')\n    plt.plot(epochs, val_rmse, marker='o', label='val_RMSE')\n    plt.xlabel('Epoch')\n    plt.ylabel('Value')\n    plt.title('Mobile-CSRNet Training Metrics')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\n\ndef evaluate_model_on_loader(model, loader, device, max_samples=None, return_all_preds=False):\n    model.eval()\n    preds = []\n    gts = []\n    img_paths = []\n    with torch.no_grad():\n        for i, (img, target) in enumerate(loader):\n            if max_samples is not None and i >= max_samples:\n                break\n            img = img.to(device)\n            target = target.to(device)\n\n            output = model(img)\n            pred_count = float(output.detach().cpu().sum().item())\n            gt_count = float(target.detach().cpu().sum().item())\n\n            preds.append(pred_count)\n            gts.append(gt_count)\n            if hasattr(loader.dataset, 'img_paths'):\n                try:\n                    img_paths.append(loader.dataset.img_paths[i])\n                except Exception:\n                    img_paths.append(None)\n            else:\n                img_paths.append(None)\n\n    preds = np.array(preds)\n    gts = np.array(gts)\n    abs_errors = np.abs(preds - gts)\n    mae = float(np.mean(abs_errors)) if len(abs_errors) > 0 else float('nan')\n    rmse = float(np.sqrt(np.mean((preds - gts)**2))) if len(abs_errors) > 0 else float('nan')\n\n    if return_all_preds:\n        return {'preds': preds, 'gts': gts, 'img_paths': img_paths, 'mae': mae, 'rmse': rmse}\n    else:\n        return mae, rmse\n\n\ndef save_predictions_csv(img_paths, preds, gts, out_csv):\n    rows = []\n    for p, pr, gt in zip(img_paths, preds, gts):\n        rows.append({'img_path': p, 'pred_count': float(pr), 'gt_count': float(gt), 'abs_error': float(abs(pr-gt))})\n    df = pd.DataFrame(rows)\n    df.to_csv(out_csv, index=False)\n\n\ndef plot_gt_vs_pred(gts, preds, out_png, title=\"GT vs Pred\"):\n    plt.figure(figsize=(6,6))\n    plt.scatter(gts, preds, alpha=0.6)\n    if len(gts)>0 and len(preds)>0:\n        mn = min(np.min(gts), np.min(preds))\n        mx = max(np.max(gts), np.max(preds))\n    else:\n        mn, mx = 0, 1\n    plt.plot([mn, mx], [mn, mx], 'r--', linewidth=1)\n    plt.xlabel('Ground Truth Count')\n    plt.ylabel('Predicted Count')\n    plt.title(title)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\n\ndef plot_residuals(gts, preds, out_png):\n    residuals = preds - gts\n    plt.figure(figsize=(8,5))\n    plt.scatter(gts, residuals, alpha=0.6)\n    plt.axhline(0, color='r', linestyle='--')\n    plt.xlabel('Ground Truth Count')\n    plt.ylabel('Residual (Pred - GT)')\n    plt.title('Residuals vs GT')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\n\ndef plot_error_histogram(errors, out_png):\n    plt.figure(figsize=(6,4))\n    plt.hist(errors, bins=30)\n    plt.xlabel('Absolute Error')\n    plt.ylabel('Frequency')\n    plt.title('Error Histogram')\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\n\n# -------------------------\n# Training pipeline for MobileNetCSRNet\n# -------------------------\n\ndef train_mobile_csrnet():\n    DATA_ROOT = \"/kaggle/input/shanghaitech/ShanghaiTech\"\n    OUT_ROOT = \"/kaggle/working/mobile_csrnet\"\n    ensure_dir(OUT_ROOT)\n\n    model_dir = os.path.join(OUT_ROOT, \"models\")\n    plots_dir = os.path.join(OUT_ROOT, \"plots\")\n    csv_dir = os.path.join(OUT_ROOT, \"csv\")\n    vis_dir = os.path.join(OUT_ROOT, \"visualizations\")\n    ensure_dir(model_dir); ensure_dir(plots_dir); ensure_dir(csv_dir); ensure_dir(vis_dir)\n\n    if not os.path.exists(DATA_ROOT):\n        print(f\"Dataset not found at {DATA_ROOT}\")\n        return\n\n    LR = 1e-5\n    BATCH_SIZE = 4\n    EPOCHS = 30\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    print(f\"Using device: {DEVICE}\")\n\n    train_ds = CSRNetDataset(DATA_ROOT, part='A', mode='train')\n    val_ds = CSRNetDataset(DATA_ROOT, part='A', mode='test')\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0)\n\n    model = MobileNetCSRNet(load_weights=True).to(DEVICE)\n\n    # Save model architecture (with shapes if possible)\n    save_model_architecture_csv(model, os.path.join(csv_dir, \"model_architecture.csv\"), device=DEVICE)\n\n    criterion = nn.MSELoss(reduction='sum')\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n    best_mae = float('inf')\n    best_epoch = -1\n    metrics_history = []\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss_sum = 0.0\n        batch_count = 0\n\n        for i, (img, target) in enumerate(train_loader):\n            img = img.to(DEVICE)\n            target = target.to(DEVICE)\n\n            output = model(img)\n            loss = criterion(output, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss_sum += float(loss.item())\n            batch_count += 1\n\n            if i % 10 == 0:\n                print(f\"Epoch {epoch+1}/{EPOCHS}, Step {i}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n\n        train_loss_avg = train_loss_sum / max(1, batch_count)\n\n        # Validation\n        model.eval()\n        mae = 0.0\n        mse_sum = 0.0\n        val_samples = 0\n\n        with torch.no_grad():\n            for img, target in val_loader:\n                img = img.to(DEVICE)\n                target = target.to(DEVICE)\n                output = model(img)\n\n                pred_count = float(output.detach().cpu().sum().item())\n                gt_count = float(target.detach().cpu().sum().item())\n\n                mae += abs(pred_count - gt_count)\n                mse_sum += (pred_count - gt_count) ** 2\n                val_samples += 1\n\n        val_mae = float(mae / val_samples) if val_samples > 0 else float('nan')\n        val_rmse = float(math.sqrt(mse_sum / val_samples)) if val_samples > 0 else float('nan')\n\n        print(f\"Epoch {epoch+1} Result: MAE: {val_mae:.2f}, RMSE: {val_rmse:.2f}, train_loss_avg: {train_loss_avg:.4f}\")\n\n        metrics_history.append({'epoch': epoch+1, 'train_loss': train_loss_avg, 'val_MAE': val_mae, 'val_RMSE': val_rmse})\n\n        # Save training metrics CSV\n        save_training_csv(metrics_history, os.path.join(csv_dir, \"mobile_csr_training_metrics.csv\"))\n\n        # Save best model\n        if val_mae < best_mae:\n            best_mae = val_mae\n            best_epoch = epoch + 1\n            best_model_path = os.path.join(model_dir, \"best_mobile_csrnet_model.pth\")\n            torch.save(model.state_dict(), best_model_path)\n            pd.DataFrame([{'best_epoch': best_epoch, 'best_MAE': best_mae, 'best_RMSE': val_rmse, 'saved_at': datetime.now().isoformat()}]).to_csv(os.path.join(csv_dir, \"best_model_metrics.csv\"), index=False)\n            print(f\"Saved best model at epoch {best_epoch} with MAE {best_mae:.4f}\")\n\n    # Save final model\n    final_model_path = os.path.join(model_dir, \"final_mobile_csrnet_model.pth\")\n    torch.save(model.state_dict(), final_model_path)\n    print(\"Saved final model:\", final_model_path)\n\n    # Plot training metrics\n    plot_training_loss(metrics_history, os.path.join(plots_dir, \"mobile_csr_training_plot.png\"))\n\n    # Load best model for evaluation\n    best_model_file = os.path.join(model_dir, \"best_mobile_csrnet_model.pth\")\n    if not os.path.exists(best_model_file):\n        print(\"Best model not found, using final model for evaluation\")\n        best_model_file = final_model_path\n\n    best_model = MobileNetCSRNet(load_weights=False).to(DEVICE)\n    best_model.load_state_dict(torch.load(best_model_file, map_location=DEVICE))\n    best_model.eval()\n\n    # Evaluate on full validation set\n    print(\"Evaluating best model on full validation set...\")\n    full_val_eval = evaluate_model_on_loader(best_model, val_loader, DEVICE, max_samples=None, return_all_preds=True)\n    final_metrics_df = pd.DataFrame([{'MAE': full_val_eval['mae'], 'RMSE': full_val_eval['rmse']}])\n    final_metrics_df.to_csv(os.path.join(csv_dir, \"final_error_matrix_best_model.csv\"), index=False)\n\n    # Sample predictions for plots\n    SAMPLE_N = 200\n    print(f\"Generating predictions on up to {SAMPLE_N} validation samples and {SAMPLE_N} train samples for plots...\")\n    val_sample_eval = evaluate_model_on_loader(best_model, val_loader, DEVICE, max_samples=SAMPLE_N, return_all_preds=True)\n    train_sample_eval = evaluate_model_on_loader(best_model, train_loader, DEVICE, max_samples=SAMPLE_N, return_all_preds=True)\n\n    sample_preds = val_sample_eval['preds']\n    sample_gts = val_sample_eval['gts']\n    sample_paths = val_sample_eval['img_paths']\n\n    save_predictions_csv(sample_paths, sample_preds, sample_gts, os.path.join(csv_dir, \"predictions_val_sample.csv\"))\n\n    if len(sample_gts) > 0:\n        plot_gt_vs_pred(sample_gts, sample_preds, os.path.join(plots_dir, \"mobile_csr_gt_vs_pred.png\"), title=\"GT vs Pred (validation sample)\")\n        n50 = min(50, len(sample_gts))\n        plot_gt_vs_pred(sample_gts[:n50], sample_preds[:n50], os.path.join(plots_dir, \"mobile_csr_gt_vs_pred_first50.png\"), title=\"GT vs Pred (first 50 val samples)\")\n        plot_residuals(sample_gts, sample_preds, os.path.join(plots_dir, \"mobile_csr_residual_plot.png\"))\n        plot_error_histogram(np.abs(sample_preds - sample_gts), os.path.join(plots_dir, \"mobile_csr_error_histogram.png\"))\n\n    combined_rows = []\n    for split_name, e in [('val', val_sample_eval), ('train', train_sample_eval)]:\n        for p, pr, gt in zip(e['img_paths'], e['preds'], e['gts']):\n            combined_rows.append({'split': split_name, 'img_path': p, 'pred_count': float(pr), 'gt_count': float(gt), 'abs_error': float(abs(pr-gt))})\n    pd.DataFrame(combined_rows).to_csv(os.path.join(csv_dir, \"predictions_combined_sample.csv\"), index=False)\n\n    print(\"All artifacts saved under:\", OUT_ROOT)\n    print(\" - Models: \", model_dir)\n    print(\" - CSVs: \", csv_dir)\n    print(\" - Plots: \", plots_dir)\n    print(\"Training finished. Best epoch:\", best_epoch, \"Best MAE:\", best_mae)\n\n\nif __name__ == '__main__':\n    train_mobile_csrnet()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:21:29.144901Z","iopub.execute_input":"2025-12-05T11:21:29.145544Z","iopub.status.idle":"2025-12-05T11:36:21.234850Z","shell.execute_reply.started":"2025-12-05T11:21:29.145514Z","shell.execute_reply":"2025-12-05T11:36:21.234149Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.6M/13.6M [00:00<00:00, 121MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30, Step 0/75, Loss: 592.9390\nEpoch 1/30, Step 10/75, Loss: 253.5897\nEpoch 1/30, Step 20/75, Loss: 196.6965\nEpoch 1/30, Step 30/75, Loss: 4026.4165\nEpoch 1/30, Step 40/75, Loss: 528.7634\nEpoch 1/30, Step 50/75, Loss: 386.6310\nEpoch 1/30, Step 60/75, Loss: 441.1340\nEpoch 1/30, Step 70/75, Loss: 495.1260\nEpoch 1 Result: MAE: 244.17, RMSE: 307.36, train_loss_avg: 1176.4197\nSaved best model at epoch 1 with MAE 244.1733\nEpoch 2/30, Step 0/75, Loss: 271.1635\nEpoch 2/30, Step 10/75, Loss: 266.4446\nEpoch 2/30, Step 20/75, Loss: 908.7999\nEpoch 2/30, Step 30/75, Loss: 1062.1809\nEpoch 2/30, Step 40/75, Loss: 2343.9758\nEpoch 2/30, Step 50/75, Loss: 88.3579\nEpoch 2/30, Step 60/75, Loss: 72.7854\nEpoch 2/30, Step 70/75, Loss: 295.0532\nEpoch 2 Result: MAE: 214.65, RMSE: 305.78, train_loss_avg: 801.8534\nSaved best model at epoch 2 with MAE 214.6466\nEpoch 3/30, Step 0/75, Loss: 152.4827\nEpoch 3/30, Step 10/75, Loss: 53.5415\nEpoch 3/30, Step 20/75, Loss: 644.6526\nEpoch 3/30, Step 30/75, Loss: 3620.4653\nEpoch 3/30, Step 40/75, Loss: 915.1504\nEpoch 3/30, Step 50/75, Loss: 79.4265\nEpoch 3/30, Step 60/75, Loss: 1153.7761\nEpoch 3/30, Step 70/75, Loss: 120.0096\nEpoch 3 Result: MAE: 131.75, RMSE: 203.57, train_loss_avg: 529.7599\nSaved best model at epoch 3 with MAE 131.7484\nEpoch 4/30, Step 0/75, Loss: 28.7693\nEpoch 4/30, Step 10/75, Loss: 348.3944\nEpoch 4/30, Step 20/75, Loss: 214.7172\nEpoch 4/30, Step 30/75, Loss: 1488.1335\nEpoch 4/30, Step 40/75, Loss: 191.8735\nEpoch 4/30, Step 50/75, Loss: 93.7547\nEpoch 4/30, Step 60/75, Loss: 245.8623\nEpoch 4/30, Step 70/75, Loss: 226.2625\nEpoch 4 Result: MAE: 210.06, RMSE: 321.23, train_loss_avg: 518.7713\nEpoch 5/30, Step 0/75, Loss: 1210.8951\nEpoch 5/30, Step 10/75, Loss: 1124.0457\nEpoch 5/30, Step 20/75, Loss: 156.1038\nEpoch 5/30, Step 30/75, Loss: 310.9399\nEpoch 5/30, Step 40/75, Loss: 866.1377\nEpoch 5/30, Step 50/75, Loss: 194.5221\nEpoch 5/30, Step 60/75, Loss: 1770.7373\nEpoch 5/30, Step 70/75, Loss: 106.6673\nEpoch 5 Result: MAE: 216.48, RMSE: 338.85, train_loss_avg: 475.0510\nEpoch 6/30, Step 0/75, Loss: 488.4117\nEpoch 6/30, Step 10/75, Loss: 571.8248\nEpoch 6/30, Step 20/75, Loss: 138.9729\nEpoch 6/30, Step 30/75, Loss: 262.2471\nEpoch 6/30, Step 40/75, Loss: 253.0532\nEpoch 6/30, Step 50/75, Loss: 322.3838\nEpoch 6/30, Step 60/75, Loss: 233.7981\nEpoch 6/30, Step 70/75, Loss: 127.2879\nEpoch 6 Result: MAE: 119.79, RMSE: 180.46, train_loss_avg: 449.4807\nSaved best model at epoch 6 with MAE 119.7935\nEpoch 7/30, Step 0/75, Loss: 99.5258\nEpoch 7/30, Step 10/75, Loss: 213.9242\nEpoch 7/30, Step 20/75, Loss: 755.3423\nEpoch 7/30, Step 30/75, Loss: 367.0953\nEpoch 7/30, Step 40/75, Loss: 53.4569\nEpoch 7/30, Step 50/75, Loss: 318.2906\nEpoch 7/30, Step 60/75, Loss: 271.6317\nEpoch 7/30, Step 70/75, Loss: 177.7969\nEpoch 7 Result: MAE: 164.84, RMSE: 240.27, train_loss_avg: 420.6602\nEpoch 8/30, Step 0/75, Loss: 166.1448\nEpoch 8/30, Step 10/75, Loss: 61.1470\nEpoch 8/30, Step 20/75, Loss: 1982.6805\nEpoch 8/30, Step 30/75, Loss: 1688.7158\nEpoch 8/30, Step 40/75, Loss: 187.7451\nEpoch 8/30, Step 50/75, Loss: 2165.8979\nEpoch 8/30, Step 60/75, Loss: 180.2293\nEpoch 8/30, Step 70/75, Loss: 481.0506\nEpoch 8 Result: MAE: 123.09, RMSE: 177.93, train_loss_avg: 395.4342\nEpoch 9/30, Step 0/75, Loss: 85.4271\nEpoch 9/30, Step 10/75, Loss: 111.8801\nEpoch 9/30, Step 20/75, Loss: 203.7412\nEpoch 9/30, Step 30/75, Loss: 251.8384\nEpoch 9/30, Step 40/75, Loss: 611.0795\nEpoch 9/30, Step 50/75, Loss: 2685.1130\nEpoch 9/30, Step 60/75, Loss: 105.8095\nEpoch 9/30, Step 70/75, Loss: 99.8485\nEpoch 9 Result: MAE: 168.65, RMSE: 244.56, train_loss_avg: 391.1150\nEpoch 10/30, Step 0/75, Loss: 1581.6086\nEpoch 10/30, Step 10/75, Loss: 241.0090\nEpoch 10/30, Step 20/75, Loss: 78.2179\nEpoch 10/30, Step 30/75, Loss: 331.8427\nEpoch 10/30, Step 40/75, Loss: 145.7691\nEpoch 10/30, Step 50/75, Loss: 1926.9048\nEpoch 10/30, Step 60/75, Loss: 553.9109\nEpoch 10/30, Step 70/75, Loss: 101.1359\nEpoch 10 Result: MAE: 131.58, RMSE: 182.10, train_loss_avg: 392.2391\nEpoch 11/30, Step 0/75, Loss: 179.3814\nEpoch 11/30, Step 10/75, Loss: 639.5622\nEpoch 11/30, Step 20/75, Loss: 1395.0132\nEpoch 11/30, Step 30/75, Loss: 1740.9900\nEpoch 11/30, Step 40/75, Loss: 535.8536\nEpoch 11/30, Step 50/75, Loss: 356.5531\nEpoch 11/30, Step 60/75, Loss: 356.4077\nEpoch 11/30, Step 70/75, Loss: 57.0556\nEpoch 11 Result: MAE: 147.92, RMSE: 198.60, train_loss_avg: 374.2745\nEpoch 12/30, Step 0/75, Loss: 70.8583\nEpoch 12/30, Step 10/75, Loss: 857.2542\nEpoch 12/30, Step 20/75, Loss: 931.0576\nEpoch 12/30, Step 30/75, Loss: 214.1469\nEpoch 12/30, Step 40/75, Loss: 141.1257\nEpoch 12/30, Step 50/75, Loss: 157.2973\nEpoch 12/30, Step 60/75, Loss: 960.8292\nEpoch 12/30, Step 70/75, Loss: 2068.9253\nEpoch 12 Result: MAE: 155.57, RMSE: 214.95, train_loss_avg: 405.6943\nEpoch 13/30, Step 0/75, Loss: 712.2034\nEpoch 13/30, Step 10/75, Loss: 99.1281\nEpoch 13/30, Step 20/75, Loss: 196.9887\nEpoch 13/30, Step 30/75, Loss: 279.3787\nEpoch 13/30, Step 40/75, Loss: 171.7697\nEpoch 13/30, Step 50/75, Loss: 191.2003\nEpoch 13/30, Step 60/75, Loss: 53.4418\nEpoch 13/30, Step 70/75, Loss: 3342.5454\nEpoch 13 Result: MAE: 121.56, RMSE: 170.33, train_loss_avg: 344.5560\nEpoch 14/30, Step 0/75, Loss: 92.3448\nEpoch 14/30, Step 10/75, Loss: 173.4148\nEpoch 14/30, Step 20/75, Loss: 175.0285\nEpoch 14/30, Step 30/75, Loss: 489.0722\nEpoch 14/30, Step 40/75, Loss: 311.0843\nEpoch 14/30, Step 50/75, Loss: 380.1318\nEpoch 14/30, Step 60/75, Loss: 86.6859\nEpoch 14/30, Step 70/75, Loss: 163.4008\nEpoch 14 Result: MAE: 135.08, RMSE: 184.89, train_loss_avg: 346.3374\nEpoch 15/30, Step 0/75, Loss: 462.1498\nEpoch 15/30, Step 10/75, Loss: 33.1622\nEpoch 15/30, Step 20/75, Loss: 1240.5706\nEpoch 15/30, Step 30/75, Loss: 659.5480\nEpoch 15/30, Step 40/75, Loss: 72.8821\nEpoch 15/30, Step 50/75, Loss: 125.0487\nEpoch 15/30, Step 60/75, Loss: 467.8484\nEpoch 15/30, Step 70/75, Loss: 215.7767\nEpoch 15 Result: MAE: 162.38, RMSE: 208.34, train_loss_avg: 319.1841\nEpoch 16/30, Step 0/75, Loss: 102.0738\nEpoch 16/30, Step 10/75, Loss: 215.5405\nEpoch 16/30, Step 20/75, Loss: 108.1563\nEpoch 16/30, Step 30/75, Loss: 440.5975\nEpoch 16/30, Step 40/75, Loss: 1441.4963\nEpoch 16/30, Step 50/75, Loss: 174.7482\nEpoch 16/30, Step 60/75, Loss: 165.2957\nEpoch 16/30, Step 70/75, Loss: 267.9350\nEpoch 16 Result: MAE: 161.59, RMSE: 224.32, train_loss_avg: 336.5428\nEpoch 17/30, Step 0/75, Loss: 113.7701\nEpoch 17/30, Step 10/75, Loss: 200.5616\nEpoch 17/30, Step 20/75, Loss: 141.2874\nEpoch 17/30, Step 30/75, Loss: 187.1808\nEpoch 17/30, Step 40/75, Loss: 232.1371\nEpoch 17/30, Step 50/75, Loss: 451.3239\nEpoch 17/30, Step 60/75, Loss: 110.2488\nEpoch 17/30, Step 70/75, Loss: 148.6304\nEpoch 17 Result: MAE: 192.20, RMSE: 257.79, train_loss_avg: 318.6965\nEpoch 18/30, Step 0/75, Loss: 583.5709\nEpoch 18/30, Step 10/75, Loss: 159.1040\nEpoch 18/30, Step 20/75, Loss: 1181.6958\nEpoch 18/30, Step 30/75, Loss: 155.7111\nEpoch 18/30, Step 40/75, Loss: 114.9593\nEpoch 18/30, Step 50/75, Loss: 193.6730\nEpoch 18/30, Step 60/75, Loss: 2833.4436\nEpoch 18/30, Step 70/75, Loss: 111.0615\nEpoch 18 Result: MAE: 112.50, RMSE: 156.15, train_loss_avg: 337.1637\nSaved best model at epoch 18 with MAE 112.4991\nEpoch 19/30, Step 0/75, Loss: 152.6702\nEpoch 19/30, Step 10/75, Loss: 1237.8379\nEpoch 19/30, Step 20/75, Loss: 89.4859\nEpoch 19/30, Step 30/75, Loss: 105.5869\nEpoch 19/30, Step 40/75, Loss: 98.9034\nEpoch 19/30, Step 50/75, Loss: 220.0973\nEpoch 19/30, Step 60/75, Loss: 305.6969\nEpoch 19/30, Step 70/75, Loss: 62.8780\nEpoch 19 Result: MAE: 111.76, RMSE: 154.05, train_loss_avg: 317.4916\nSaved best model at epoch 19 with MAE 111.7601\nEpoch 20/30, Step 0/75, Loss: 89.4289\nEpoch 20/30, Step 10/75, Loss: 467.8922\nEpoch 20/30, Step 20/75, Loss: 147.9274\nEpoch 20/30, Step 30/75, Loss: 202.7204\nEpoch 20/30, Step 40/75, Loss: 534.7855\nEpoch 20/30, Step 50/75, Loss: 442.7438\nEpoch 20/30, Step 60/75, Loss: 1205.3905\nEpoch 20/30, Step 70/75, Loss: 137.6589\nEpoch 20 Result: MAE: 118.25, RMSE: 158.80, train_loss_avg: 334.4050\nEpoch 21/30, Step 0/75, Loss: 303.5373\nEpoch 21/30, Step 10/75, Loss: 163.3578\nEpoch 21/30, Step 20/75, Loss: 500.0837\nEpoch 21/30, Step 30/75, Loss: 202.5795\nEpoch 21/30, Step 40/75, Loss: 104.9901\nEpoch 21/30, Step 50/75, Loss: 467.6898\nEpoch 21/30, Step 60/75, Loss: 340.1445\nEpoch 21/30, Step 70/75, Loss: 88.1839\nEpoch 21 Result: MAE: 121.44, RMSE: 165.54, train_loss_avg: 328.5714\nEpoch 22/30, Step 0/75, Loss: 95.1150\nEpoch 22/30, Step 10/75, Loss: 93.7741\nEpoch 22/30, Step 20/75, Loss: 231.2499\nEpoch 22/30, Step 30/75, Loss: 150.1752\nEpoch 22/30, Step 40/75, Loss: 219.3268\nEpoch 22/30, Step 50/75, Loss: 389.5059\nEpoch 22/30, Step 60/75, Loss: 169.2819\nEpoch 22/30, Step 70/75, Loss: 68.3756\nEpoch 22 Result: MAE: 121.59, RMSE: 164.39, train_loss_avg: 297.2713\nEpoch 23/30, Step 0/75, Loss: 240.6249\nEpoch 23/30, Step 10/75, Loss: 212.4457\nEpoch 23/30, Step 20/75, Loss: 231.6217\nEpoch 23/30, Step 30/75, Loss: 132.1735\nEpoch 23/30, Step 40/75, Loss: 351.5330\nEpoch 23/30, Step 50/75, Loss: 66.5228\nEpoch 23/30, Step 60/75, Loss: 78.2222\nEpoch 23/30, Step 70/75, Loss: 219.2533\nEpoch 23 Result: MAE: 109.41, RMSE: 149.92, train_loss_avg: 302.9204\nSaved best model at epoch 23 with MAE 109.4120\nEpoch 24/30, Step 0/75, Loss: 123.8861\nEpoch 24/30, Step 10/75, Loss: 534.3348\nEpoch 24/30, Step 20/75, Loss: 265.6318\nEpoch 24/30, Step 30/75, Loss: 487.3947\nEpoch 24/30, Step 40/75, Loss: 230.8122\nEpoch 24/30, Step 50/75, Loss: 141.9111\nEpoch 24/30, Step 60/75, Loss: 100.6412\nEpoch 24/30, Step 70/75, Loss: 403.9418\nEpoch 24 Result: MAE: 117.97, RMSE: 159.40, train_loss_avg: 265.3272\nEpoch 25/30, Step 0/75, Loss: 239.1528\nEpoch 25/30, Step 10/75, Loss: 315.5035\nEpoch 25/30, Step 20/75, Loss: 80.0055\nEpoch 25/30, Step 30/75, Loss: 52.6904\nEpoch 25/30, Step 40/75, Loss: 348.3926\nEpoch 25/30, Step 50/75, Loss: 162.2617\nEpoch 25/30, Step 60/75, Loss: 86.8824\nEpoch 25/30, Step 70/75, Loss: 111.0578\nEpoch 25 Result: MAE: 109.74, RMSE: 148.26, train_loss_avg: 278.2398\nEpoch 26/30, Step 0/75, Loss: 385.2168\nEpoch 26/30, Step 10/75, Loss: 257.5562\nEpoch 26/30, Step 20/75, Loss: 372.3678\nEpoch 26/30, Step 30/75, Loss: 32.7719\nEpoch 26/30, Step 40/75, Loss: 1648.4700\nEpoch 26/30, Step 50/75, Loss: 308.8208\nEpoch 26/30, Step 60/75, Loss: 89.8717\nEpoch 26/30, Step 70/75, Loss: 116.0469\nEpoch 26 Result: MAE: 148.58, RMSE: 196.22, train_loss_avg: 286.3504\nEpoch 27/30, Step 0/75, Loss: 120.0477\nEpoch 27/30, Step 10/75, Loss: 36.4745\nEpoch 27/30, Step 20/75, Loss: 42.3703\nEpoch 27/30, Step 30/75, Loss: 119.7961\nEpoch 27/30, Step 40/75, Loss: 199.5866\nEpoch 27/30, Step 50/75, Loss: 204.2509\nEpoch 27/30, Step 60/75, Loss: 152.2374\nEpoch 27/30, Step 70/75, Loss: 151.5321\nEpoch 27 Result: MAE: 109.67, RMSE: 148.33, train_loss_avg: 319.8272\nEpoch 28/30, Step 0/75, Loss: 821.5562\nEpoch 28/30, Step 10/75, Loss: 260.4871\nEpoch 28/30, Step 20/75, Loss: 516.8646\nEpoch 28/30, Step 30/75, Loss: 249.0458\nEpoch 28/30, Step 40/75, Loss: 136.7452\nEpoch 28/30, Step 50/75, Loss: 41.9766\nEpoch 28/30, Step 60/75, Loss: 196.5635\nEpoch 28/30, Step 70/75, Loss: 69.2521\nEpoch 28 Result: MAE: 112.90, RMSE: 156.29, train_loss_avg: 272.7161\nEpoch 29/30, Step 0/75, Loss: 303.5536\nEpoch 29/30, Step 10/75, Loss: 103.4324\nEpoch 29/30, Step 20/75, Loss: 187.3695\nEpoch 29/30, Step 30/75, Loss: 174.3451\nEpoch 29/30, Step 40/75, Loss: 406.5988\nEpoch 29/30, Step 50/75, Loss: 167.1484\nEpoch 29/30, Step 60/75, Loss: 200.4029\nEpoch 29/30, Step 70/75, Loss: 161.9651\nEpoch 29 Result: MAE: 135.34, RMSE: 179.29, train_loss_avg: 288.7112\nEpoch 30/30, Step 0/75, Loss: 122.9302\nEpoch 30/30, Step 10/75, Loss: 75.8305\nEpoch 30/30, Step 20/75, Loss: 220.6806\nEpoch 30/30, Step 30/75, Loss: 217.5481\nEpoch 30/30, Step 40/75, Loss: 348.4211\nEpoch 30/30, Step 50/75, Loss: 90.4776\nEpoch 30/30, Step 60/75, Loss: 152.8024\nEpoch 30/30, Step 70/75, Loss: 192.0135\nEpoch 30 Result: MAE: 110.25, RMSE: 148.75, train_loss_avg: 266.9957\nSaved final model: /kaggle/working/mobile_csrnet/models/final_mobile_csrnet_model.pth\nEvaluating best model on full validation set...\nGenerating predictions on up to 200 validation samples and 200 train samples for plots...\nAll artifacts saved under: /kaggle/working/mobile_csrnet\n - Models:  /kaggle/working/mobile_csrnet/models\n - CSVs:  /kaggle/working/mobile_csrnet/csv\n - Plots:  /kaggle/working/mobile_csrnet/plots\nTraining finished. Best epoch: 23 Best MAE: 109.41203482072432\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}